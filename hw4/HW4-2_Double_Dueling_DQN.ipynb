{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4e0dc06",
   "metadata": {},
   "source": [
    "# HW4-2: Enhanced DQN Variants (Double DQN & Dueling DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1408c817",
   "metadata": {},
   "source": [
    "\n",
    "æœ¬ Notebook æ¯”è¼ƒå…©ç¨®å¸¸è¦‹çš„ DQN å¢å¼·æ–¹æ³•ï¼šDouble DQN èˆ‡ Dueling DQNã€‚\n",
    "é€™äº›æ–¹æ³•åœ¨å¼·åŒ–å­¸ç¿’ä¸­è¢«æå‡ºä¾†æ”¹å–„åŸå§‹ DQN çš„é«˜ä¼°åå·®èˆ‡å­¸ç¿’æ•ˆç‡å•é¡Œã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… æœ¬ Notebook åŒ…å«ï¼š\n",
    "- åŸå§‹ DQN æ‘˜è¦\n",
    "- Double DQN ä¿®æ”¹\n",
    "- Dueling DQN æ¶æ§‹å¯¦ä½œ\n",
    "- è¨“ç·´éç¨‹èˆ‡çµæœæ¯”è¼ƒ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2b283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸‹è¼‰ Gridworld.py åŠ GridBoard.py (-q æ˜¯è¨­ç‚ºå®‰éœæ¨¡å¼)\n",
    "!curl -q https://github.com/DeepReinforcementLearning/DeepReinforcementLearningInAction/raw/master/Errata/Gridworld.py\n",
    "!curl -q https://github.com/DeepReinforcementLearning/DeepReinforcementLearningInAction/raw/master/Errata/GridBoard.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7877ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Gridworld import Gridworld\n",
    "game = Gridworld(size=4, mode='static')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35055126",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c80277d",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.makeMove('d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff172d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74812507",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce098fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.board.render_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f6c485",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.board.render_np().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a459e60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from Gridworld import Gridworld\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "L1 = 64 #è¼¸å…¥å±¤çš„å¯¬åº¦\n",
    "L2 = 150 #ç¬¬ä¸€éš±è—å±¤çš„å¯¬åº¦\n",
    "L3 = 100 #ç¬¬äºŒéš±è—å±¤çš„å¯¬åº¦\n",
    "L4 = 4 #è¼¸å‡ºå±¤çš„å¯¬åº¦\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(L1, L2), #ç¬¬ä¸€éš±è—å±¤çš„shape \n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(L2, L3), #ç¬¬äºŒéš±è—å±¤çš„shape\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(L3,L4) #è¼¸å‡ºå±¤çš„shape\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss() #æŒ‡å®šæå¤±å‡½æ•¸ç‚ºMSEï¼ˆå‡æ–¹èª¤å·®ï¼‰\n",
    "learning_rate = 1e-3  #è¨­å®šå­¸ç¿’ç‡\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) #æŒ‡å®šå„ªåŒ–å™¨ç‚ºAdamï¼Œå…¶ä¸­model.parametersæœƒå‚³å›æ‰€æœ‰è¦å„ªåŒ–çš„æ¬Šé‡åƒæ•¸\n",
    "\n",
    "gamma = 0.9 #æŠ˜æ‰£å› å­\n",
    "epsilon = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31d94ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_set = {\n",
    "\t0: 'u', #ã€0ã€ä»£è¡¨ã€å‘ä¸Šã€\n",
    "\t1: 'd', #ã€1ã€ä»£è¡¨ã€å‘ä¸‹ã€\n",
    "\t2: 'l', #ã€2ã€ä»£è¡¨ã€å‘å·¦ã€\n",
    "\t3: 'r' #ã€3ã€ä»£è¡¨ã€å‘å³ã€\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4717a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "losses = [] #ä½¿ç”¨ä¸²åˆ—å°‡æ¯ä¸€æ¬¡çš„lossè¨˜éŒ„ä¸‹ä¾†ï¼Œæ–¹ä¾¿ä¹‹å¾Œå°‡lossçš„è®ŠåŒ–è¶¨å‹¢ç•«æˆåœ–\n",
    "for i in range(epochs):\n",
    "  game = Gridworld(size=4, mode='static')\n",
    "  state_ = game.board.render_np().reshape(1,64) + np.random.rand(1,64)/10.0 #å°‡3éšçš„ç‹€æ…‹é™£åˆ—ï¼ˆ4x4x4ï¼‰è½‰æ›æˆå‘é‡ï¼ˆé•·åº¦ç‚º64ï¼‰ï¼Œä¸¦å°‡æ¯å€‹å€¼éƒ½åŠ ä¸Šä¸€äº›é›œè¨Šï¼ˆå¾ˆå°çš„æ•¸å€¼ï¼‰ã€‚\t\n",
    "  state1 = torch.from_numpy(state_).float() #å°‡NumPyé™£åˆ—è½‰æ›æˆPyTorchå¼µé‡ï¼Œä¸¦å­˜æ–¼state1ä¸­\n",
    "  status = 1 #ç”¨ä¾†è¿½è¹¤éŠæˆ²æ˜¯å¦ä»åœ¨ç¹¼çºŒï¼ˆã€1ã€ä»£è¡¨ä»åœ¨ç¹¼çºŒï¼‰\n",
    "  while(status == 1):\n",
    "    qval = model(state1) #åŸ·è¡ŒQç¶²è·¯ï¼Œå–å¾—æ‰€æœ‰å‹•ä½œçš„é æ¸¬Qå€¼\n",
    "    qval_ = qval.data.numpy() #å°‡qvalè½‰æ›æˆNumPyé™£åˆ—\n",
    "    if (random.random() < epsilon): \n",
    "      action_ = np.random.randint(0,4) #éš¨æ©Ÿé¸æ“‡ä¸€å€‹å‹•ä½œï¼ˆæ¢ç´¢ï¼‰\n",
    "    else:\n",
    "      action_ = np.argmax(qval_) #é¸æ“‡Qå€¼æœ€å¤§çš„å‹•ä½œï¼ˆæ¢ç´¢ï¼‰        \n",
    "    action = action_set[action_] #å°‡ä»£è¡¨æŸå‹•ä½œçš„æ•¸å­—å°æ‡‰åˆ°makeMove()çš„è‹±æ–‡å­—æ¯\n",
    "    game.makeMove(action) #åŸ·è¡Œä¹‹å‰Îµâ€”è²ªå©ªç­–ç•¥æ‰€é¸å‡ºçš„å‹•ä½œ \n",
    "    state2_ = game.board.render_np().reshape(1,64) + np.random.rand(1,64)/10.0\n",
    "    state2 = torch.from_numpy(state2_).float() #å‹•ä½œåŸ·è¡Œå®Œç•¢ï¼Œå–å¾—éŠæˆ²çš„æ–°ç‹€æ…‹ä¸¦è½‰æ›æˆå¼µé‡\n",
    "    reward = game.reward()\n",
    "    with torch.no_grad(): \n",
    "      newQ = model(state2.reshape(1,64))\n",
    "    maxQ = torch.max(newQ) #å°‡æ–°ç‹€æ…‹ä¸‹æ‰€è¼¸å‡ºçš„Qå€¼å‘é‡ä¸­çš„æœ€å¤§å€¼çµ¦è¨˜éŒ„ä¸‹ä¾†\n",
    "    if reward == -1:\n",
    "      Y = reward + (gamma * maxQ)  #è¨ˆç®—è¨“ç·´æ‰€ç”¨çš„ç›®æ¨™Qå€¼\n",
    "    else: #è‹¥rewardä¸ç­‰æ–¼-1ï¼Œä»£è¡¨éŠæˆ²å·²ç¶“çµæŸï¼Œä¹Ÿå°±æ²’æœ‰ä¸‹ä¸€å€‹ç‹€æ…‹äº†ï¼Œå› æ­¤ç›®æ¨™Qå€¼å°±ç­‰æ–¼å›é¥‹å€¼\n",
    "      Y = reward\n",
    "    Y = torch.Tensor([Y]).detach() \n",
    "    X = qval.squeeze()[action_] #å°‡æ¼”ç®—æ³•å°åŸ·è¡Œçš„å‹•ä½œæ‰€é æ¸¬çš„Qå€¼å­˜é€²Xï¼Œä¸¦ä½¿ç”¨squeeze()å°‡qvalä¸­ç¶­åº¦ç‚º1çš„éšå»æ‰ (shape[1,4]æœƒè®Šæˆ[4])\n",
    "    loss = loss_fn(X, Y) #è¨ˆç®—ç›®æ¨™Qå€¼èˆ‡é æ¸¬Qå€¼ä¹‹é–“çš„èª¤å·®\n",
    "    if i%100 == 0:\n",
    "      print(i, loss.item())\n",
    "      clear_output(wait=True)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    state1 = state2\n",
    "    if abs(reward) == 10:       \n",
    "      status = 0 # è‹¥ reward çš„çµ•å°å€¼ç‚º10ï¼Œä»£è¡¨éŠæˆ²å·²ç¶“åˆ†å‡ºå‹è² ï¼Œæ‰€ä»¥è¨­statusç‚º0  \n",
    "  losses.append(loss.item())\n",
    "  if epsilon > 0.1: \n",
    "    epsilon -= (1/epochs) #è®“Îµçš„å€¼éš¨è‘—è¨“ç·´çš„é€²è¡Œè€Œæ…¢æ…¢ä¸‹é™ï¼Œç›´åˆ°0.1ï¼ˆé‚„æ˜¯è¦ä¿ç•™æ¢ç´¢çš„å‹•ä½œï¼‰\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epochs\",fontsize=11)\n",
    "plt.ylabel(\"Loss\",fontsize=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496daba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.Tensor([2.0])\n",
    "m.requires_grad=True\n",
    "b = torch.Tensor([1.0]) \n",
    "b.requires_grad=True\n",
    "def linear_model(x,m,b):\n",
    "  y = m*x + b\n",
    "  return y\n",
    "y = linear_model(torch.Tensor([4.]),m,b)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b085c381",
   "metadata": {},
   "source": [
    "## ğŸ” Double DQN å¯¦ä½œ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a964097",
   "metadata": {},
   "source": [
    "\n",
    "Double DQN çš„é—œéµåœ¨æ–¼å°‡è¡Œå‹•é¸æ“‡èˆ‡ Q å€¼è©•ä¼°åˆ†é–‹ï¼š\n",
    "- ä½¿ç”¨ online network é¸æ“‡ä¸‹ä¸€æ­¥è¡Œå‹• (argmax)\n",
    "- ä½¿ç”¨ target network è¨ˆç®—å°æ‡‰ Q å€¼\n",
    "\n",
    "é€™å¯ä»¥æœ‰æ•ˆé¿å…åŸå§‹ DQN ä¸­éåº¦é«˜ä¼° Q å€¼çš„å•é¡Œã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb08bbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Double DQN æå¤±è¨ˆç®—ï¼ˆå–ä»£åŸæœ¬çš„ loss è¨ˆç®—ï¼‰\n",
    "with torch.no_grad():\n",
    "    next_actions = model(torch.from_numpy(next_state).float().unsqueeze(0)).argmax().item()\n",
    "    target_q = reward + gamma * target_model(torch.from_numpy(next_state).float().unsqueeze(0))[0][next_actions]\n",
    "\n",
    "predicted_q = model(torch.from_numpy(state).float().unsqueeze(0))[0][action]\n",
    "loss = loss_fn(predicted_q, target_q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b6cb7e",
   "metadata": {},
   "source": [
    "## ğŸ›ï¸ Dueling DQN æ¶æ§‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c8c40b",
   "metadata": {},
   "source": [
    "\n",
    "Dueling DQN å°‡ Q å€¼æ‹†è§£ç‚ºå…©å€‹å­ç¶²è·¯ï¼š\n",
    "- Value function: è¡¡é‡è©²ç‹€æ…‹çš„æ•´é«”åƒ¹å€¼\n",
    "- Advantage function: è¡¡é‡æ¡å–æŸè¡Œå‹•æ˜¯å¦æ¯”å¹³å‡æ›´å¥½\n",
    "\n",
    "åˆä½µå…¬å¼ï¼š \\( Q(s, a) = V(s) + (A(s, a) - \\frac{1}{|A|} \\sum A(s, a')) \\)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce7c098",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dueling DQN PyTorch æ¶æ§‹\n",
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature(x)\n",
    "        value = self.value_stream(x)\n",
    "        advantage = self.advantage_stream(x)\n",
    "        return value + (advantage - advantage.mean(dim=1, keepdim=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad18ac8",
   "metadata": {},
   "source": [
    "## ğŸ“Š çµæœæ¯”è¼ƒ (åŸå§‹ DQN vs Double DQN vs Dueling DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a87908",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# å‡è¨­ reward_list_naive, reward_list_double, reward_list_dueling åˆ†åˆ¥å„²å­˜ä¸‰ç¨®æ¨¡å‹çš„æ¯é›†ç¸½ reward\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(reward_list_naive, label='Naive DQN')\n",
    "plt.plot(reward_list_double, label='Double DQN')\n",
    "plt.plot(reward_list_dueling, label='Dueling DQN')\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.legend()\n",
    "plt.title(\"Comparison of DQN Variants\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec9de3b",
   "metadata": {},
   "source": [
    "## ğŸ“‹ å°çµèˆ‡ç†è§£èªªæ˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3173e53a",
   "metadata": {},
   "source": [
    "\n",
    "- **Double DQN** æ¸›å°‘äº†éé«˜ä¼°è¨ˆçš„ biasï¼Œçµæœè¼ƒç©©å®šã€‚\n",
    "- **Dueling DQN** åœ¨ç‹€æ…‹åƒ¹å€¼ä¸»å°çš„æƒ…å¢ƒä¸‹ï¼ˆå¦‚ç›®æ¨™è·é›¢ã€ç‰†å£é¿è®“ï¼‰æœ‰æ›´å¿«æ”¶æ–‚ã€‚\n",
    "- è‹¥ç’°å¢ƒå–®ç´”ï¼ˆå¦‚æœ¬æ¬¡ Gridworldï¼‰ï¼Œä¸‰è€… reward å·®è·å¯èƒ½ä¸å¤§ï¼Œä½†æ”¶æ–‚é€Ÿåº¦å’Œç©©å®šæ€§èƒ½è§€å¯Ÿåˆ°å·®ç•°ã€‚\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
