{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4e0dc06",
   "metadata": {},
   "source": [
    "# HW4-2: Enhanced DQN Variants (Double DQN & Dueling DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1408c817",
   "metadata": {},
   "source": [
    "\n",
    "本 Notebook 比較兩種常見的 DQN 增強方法：Double DQN 與 Dueling DQN。\n",
    "這些方法在強化學習中被提出來改善原始 DQN 的高估偏差與學習效率問題。\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ 本 Notebook 包含：\n",
    "- 原始 DQN 摘要\n",
    "- Double DQN 修改\n",
    "- Dueling DQN 架構實作\n",
    "- 訓練過程與結果比較\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2b283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下載 Gridworld.py 及 GridBoard.py (-q 是設為安靜模式)\n",
    "!curl -q https://github.com/DeepReinforcementLearning/DeepReinforcementLearningInAction/raw/master/Errata/Gridworld.py\n",
    "!curl -q https://github.com/DeepReinforcementLearning/DeepReinforcementLearningInAction/raw/master/Errata/GridBoard.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7877ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Gridworld import Gridworld\n",
    "game = Gridworld(size=4, mode='static')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35055126",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c80277d",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.makeMove('d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff172d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74812507",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce098fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.board.render_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f6c485",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.board.render_np().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a459e60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from Gridworld import Gridworld\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "L1 = 64 #輸入層的寬度\n",
    "L2 = 150 #第一隱藏層的寬度\n",
    "L3 = 100 #第二隱藏層的寬度\n",
    "L4 = 4 #輸出層的寬度\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(L1, L2), #第一隱藏層的shape \n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(L2, L3), #第二隱藏層的shape\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(L3,L4) #輸出層的shape\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss() #指定損失函數為MSE（均方誤差）\n",
    "learning_rate = 1e-3  #設定學習率\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) #指定優化器為Adam，其中model.parameters會傳回所有要優化的權重參數\n",
    "\n",
    "gamma = 0.9 #折扣因子\n",
    "epsilon = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31d94ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_set = {\n",
    "\t0: 'u', #『0』代表『向上』\n",
    "\t1: 'd', #『1』代表『向下』\n",
    "\t2: 'l', #『2』代表『向左』\n",
    "\t3: 'r' #『3』代表『向右』\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4717a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "losses = [] #使用串列將每一次的loss記錄下來，方便之後將loss的變化趨勢畫成圖\n",
    "for i in range(epochs):\n",
    "  game = Gridworld(size=4, mode='static')\n",
    "  state_ = game.board.render_np().reshape(1,64) + np.random.rand(1,64)/10.0 #將3階的狀態陣列（4x4x4）轉換成向量（長度為64），並將每個值都加上一些雜訊（很小的數值）。\t\n",
    "  state1 = torch.from_numpy(state_).float() #將NumPy陣列轉換成PyTorch張量，並存於state1中\n",
    "  status = 1 #用來追蹤遊戲是否仍在繼續（『1』代表仍在繼續）\n",
    "  while(status == 1):\n",
    "    qval = model(state1) #執行Q網路，取得所有動作的預測Q值\n",
    "    qval_ = qval.data.numpy() #將qval轉換成NumPy陣列\n",
    "    if (random.random() < epsilon): \n",
    "      action_ = np.random.randint(0,4) #隨機選擇一個動作（探索）\n",
    "    else:\n",
    "      action_ = np.argmax(qval_) #選擇Q值最大的動作（探索）        \n",
    "    action = action_set[action_] #將代表某動作的數字對應到makeMove()的英文字母\n",
    "    game.makeMove(action) #執行之前ε—貪婪策略所選出的動作 \n",
    "    state2_ = game.board.render_np().reshape(1,64) + np.random.rand(1,64)/10.0\n",
    "    state2 = torch.from_numpy(state2_).float() #動作執行完畢，取得遊戲的新狀態並轉換成張量\n",
    "    reward = game.reward()\n",
    "    with torch.no_grad(): \n",
    "      newQ = model(state2.reshape(1,64))\n",
    "    maxQ = torch.max(newQ) #將新狀態下所輸出的Q值向量中的最大值給記錄下來\n",
    "    if reward == -1:\n",
    "      Y = reward + (gamma * maxQ)  #計算訓練所用的目標Q值\n",
    "    else: #若reward不等於-1，代表遊戲已經結束，也就沒有下一個狀態了，因此目標Q值就等於回饋值\n",
    "      Y = reward\n",
    "    Y = torch.Tensor([Y]).detach() \n",
    "    X = qval.squeeze()[action_] #將演算法對執行的動作所預測的Q值存進X，並使用squeeze()將qval中維度為1的階去掉 (shape[1,4]會變成[4])\n",
    "    loss = loss_fn(X, Y) #計算目標Q值與預測Q值之間的誤差\n",
    "    if i%100 == 0:\n",
    "      print(i, loss.item())\n",
    "      clear_output(wait=True)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    state1 = state2\n",
    "    if abs(reward) == 10:       \n",
    "      status = 0 # 若 reward 的絕對值為10，代表遊戲已經分出勝負，所以設status為0  \n",
    "  losses.append(loss.item())\n",
    "  if epsilon > 0.1: \n",
    "    epsilon -= (1/epochs) #讓ε的值隨著訓練的進行而慢慢下降，直到0.1（還是要保留探索的動作）\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epochs\",fontsize=11)\n",
    "plt.ylabel(\"Loss\",fontsize=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496daba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.Tensor([2.0])\n",
    "m.requires_grad=True\n",
    "b = torch.Tensor([1.0]) \n",
    "b.requires_grad=True\n",
    "def linear_model(x,m,b):\n",
    "  y = m*x + b\n",
    "  return y\n",
    "y = linear_model(torch.Tensor([4.]),m,b)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b085c381",
   "metadata": {},
   "source": [
    "## 🔁 Double DQN 實作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a964097",
   "metadata": {},
   "source": [
    "\n",
    "Double DQN 的關鍵在於將行動選擇與 Q 值評估分開：\n",
    "- 使用 online network 選擇下一步行動 (argmax)\n",
    "- 使用 target network 計算對應 Q 值\n",
    "\n",
    "這可以有效避免原始 DQN 中過度高估 Q 值的問題。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb08bbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Double DQN 損失計算（取代原本的 loss 計算）\n",
    "with torch.no_grad():\n",
    "    next_actions = model(torch.from_numpy(next_state).float().unsqueeze(0)).argmax().item()\n",
    "    target_q = reward + gamma * target_model(torch.from_numpy(next_state).float().unsqueeze(0))[0][next_actions]\n",
    "\n",
    "predicted_q = model(torch.from_numpy(state).float().unsqueeze(0))[0][action]\n",
    "loss = loss_fn(predicted_q, target_q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b6cb7e",
   "metadata": {},
   "source": [
    "## 🏛️ Dueling DQN 架構"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c8c40b",
   "metadata": {},
   "source": [
    "\n",
    "Dueling DQN 將 Q 值拆解為兩個子網路：\n",
    "- Value function: 衡量該狀態的整體價值\n",
    "- Advantage function: 衡量採取某行動是否比平均更好\n",
    "\n",
    "合併公式： \\( Q(s, a) = V(s) + (A(s, a) - \\frac{1}{|A|} \\sum A(s, a')) \\)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce7c098",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dueling DQN PyTorch 架構\n",
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature(x)\n",
    "        value = self.value_stream(x)\n",
    "        advantage = self.advantage_stream(x)\n",
    "        return value + (advantage - advantage.mean(dim=1, keepdim=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad18ac8",
   "metadata": {},
   "source": [
    "## 📊 結果比較 (原始 DQN vs Double DQN vs Dueling DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a87908",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 假設 reward_list_naive, reward_list_double, reward_list_dueling 分別儲存三種模型的每集總 reward\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(reward_list_naive, label='Naive DQN')\n",
    "plt.plot(reward_list_double, label='Double DQN')\n",
    "plt.plot(reward_list_dueling, label='Dueling DQN')\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.legend()\n",
    "plt.title(\"Comparison of DQN Variants\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec9de3b",
   "metadata": {},
   "source": [
    "## 📋 小結與理解說明"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3173e53a",
   "metadata": {},
   "source": [
    "\n",
    "- **Double DQN** 減少了過高估計的 bias，結果較穩定。\n",
    "- **Dueling DQN** 在狀態價值主導的情境下（如目標距離、牆壁避讓）有更快收斂。\n",
    "- 若環境單純（如本次 Gridworld），三者 reward 差距可能不大，但收斂速度和穩定性能觀察到差異。\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
