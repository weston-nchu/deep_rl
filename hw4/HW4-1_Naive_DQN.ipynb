{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3a78fb9",
   "metadata": {},
   "source": [
    "# HW4-1: Naive DQN for Static Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7639eb4",
   "metadata": {},
   "source": [
    "\n",
    "本 Notebook 展示基礎 DQN（Deep Q-Learning Network）的實作，並應用於簡單的 Gridworld 環境進行學習。\n",
    "\n",
    "### 包含內容：\n",
    "- 環境初始化（Gridworld）\n",
    "- Naive DQN 網路架構（使用 PyTorch）\n",
    "- Experience Replay 機制\n",
    "- 訓練流程與損失繪圖\n",
    "- 理解說明\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f745c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下載 Gridworld.py 及 GridBoard.py (-q 是設為安靜模式)\n",
    "!curl -q https://github.com/DeepReinforcementLearning/DeepReinforcementLearningInAction/raw/master/Errata/Gridworld.py\n",
    "!curl -q https://github.com/DeepReinforcementLearning/DeepReinforcementLearningInAction/raw/master/Errata/GridBoard.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ad4a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Gridworld import Gridworld\n",
    "game = Gridworld(size=4, mode='static')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce23f3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad381b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.makeMove('d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa969765",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cd5a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d223751",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.board.render_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7351dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.board.render_np().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5237816b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from Gridworld import Gridworld\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "L1 = 64 #輸入層的寬度\n",
    "L2 = 150 #第一隱藏層的寬度\n",
    "L3 = 100 #第二隱藏層的寬度\n",
    "L4 = 4 #輸出層的寬度\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(L1, L2), #第一隱藏層的shape \n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(L2, L3), #第二隱藏層的shape\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(L3,L4) #輸出層的shape\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss() #指定損失函數為MSE（均方誤差）\n",
    "learning_rate = 1e-3  #設定學習率\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) #指定優化器為Adam，其中model.parameters會傳回所有要優化的權重參數\n",
    "\n",
    "gamma = 0.9 #折扣因子\n",
    "epsilon = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d00565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_set = {\n",
    "\t0: 'u', #『0』代表『向上』\n",
    "\t1: 'd', #『1』代表『向下』\n",
    "\t2: 'l', #『2』代表『向左』\n",
    "\t3: 'r' #『3』代表『向右』\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb521737",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "losses = [] #使用串列將每一次的loss記錄下來，方便之後將loss的變化趨勢畫成圖\n",
    "for i in range(epochs):\n",
    "  game = Gridworld(size=4, mode='static')\n",
    "  state_ = game.board.render_np().reshape(1,64) + np.random.rand(1,64)/10.0 #將3階的狀態陣列（4x4x4）轉換成向量（長度為64），並將每個值都加上一些雜訊（很小的數值）。\t\n",
    "  state1 = torch.from_numpy(state_).float() #將NumPy陣列轉換成PyTorch張量，並存於state1中\n",
    "  status = 1 #用來追蹤遊戲是否仍在繼續（『1』代表仍在繼續）\n",
    "  while(status == 1):\n",
    "    qval = model(state1) #執行Q網路，取得所有動作的預測Q值\n",
    "    qval_ = qval.data.numpy() #將qval轉換成NumPy陣列\n",
    "    if (random.random() < epsilon): \n",
    "      action_ = np.random.randint(0,4) #隨機選擇一個動作（探索）\n",
    "    else:\n",
    "      action_ = np.argmax(qval_) #選擇Q值最大的動作（探索）        \n",
    "    action = action_set[action_] #將代表某動作的數字對應到makeMove()的英文字母\n",
    "    game.makeMove(action) #執行之前ε—貪婪策略所選出的動作 \n",
    "    state2_ = game.board.render_np().reshape(1,64) + np.random.rand(1,64)/10.0\n",
    "    state2 = torch.from_numpy(state2_).float() #動作執行完畢，取得遊戲的新狀態並轉換成張量\n",
    "    reward = game.reward()\n",
    "    with torch.no_grad(): \n",
    "      newQ = model(state2.reshape(1,64))\n",
    "    maxQ = torch.max(newQ) #將新狀態下所輸出的Q值向量中的最大值給記錄下來\n",
    "    if reward == -1:\n",
    "      Y = reward + (gamma * maxQ)  #計算訓練所用的目標Q值\n",
    "    else: #若reward不等於-1，代表遊戲已經結束，也就沒有下一個狀態了，因此目標Q值就等於回饋值\n",
    "      Y = reward\n",
    "    Y = torch.Tensor([Y]).detach() \n",
    "    X = qval.squeeze()[action_] #將演算法對執行的動作所預測的Q值存進X，並使用squeeze()將qval中維度為1的階去掉 (shape[1,4]會變成[4])\n",
    "    loss = loss_fn(X, Y) #計算目標Q值與預測Q值之間的誤差\n",
    "    if i%100 == 0:\n",
    "      print(i, loss.item())\n",
    "      clear_output(wait=True)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    state1 = state2\n",
    "    if abs(reward) == 10:       \n",
    "      status = 0 # 若 reward 的絕對值為10，代表遊戲已經分出勝負，所以設status為0  \n",
    "  losses.append(loss.item())\n",
    "  if epsilon > 0.1: \n",
    "    epsilon -= (1/epochs) #讓ε的值隨著訓練的進行而慢慢下降，直到0.1（還是要保留探索的動作）\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epochs\",fontsize=11)\n",
    "plt.ylabel(\"Loss\",fontsize=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5b22f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.Tensor([2.0])\n",
    "m.requires_grad=True\n",
    "b = torch.Tensor([1.0]) \n",
    "b.requires_grad=True\n",
    "def linear_model(x,m,b):\n",
    "  y = m*x + b\n",
    "  return y\n",
    "y = linear_model(torch.Tensor([4.]),m,b)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c095539",
   "metadata": {},
   "source": [
    "## 🔁 DQN 訓練流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec272438",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363716fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(): \n",
    "  y = linear_model(torch.Tensor([4.]),m,b)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c317f242",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9e3817",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = linear_model(torch.Tensor([4.]),m,b)\n",
    "y.backward()\n",
    "m.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83131a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a10fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, mode='static', display=True):\n",
    "    i = 0\n",
    "    test_game = Gridworld(size=4, mode=mode) #產生一場測試遊戲\n",
    "    state_ = test_game.board.render_np().reshape(1,64) + np.random.rand(1,64)/10.0\n",
    "    state = torch.from_numpy(state_).float()\n",
    "    if display:\n",
    "      print(\"Initial State:\")\n",
    "      print(test_game.display())\n",
    "    status = 1\n",
    "    while(status == 1): #遊戲仍在進行\n",
    "      qval = model(state)\n",
    "      qval_ = qval.data.numpy()\n",
    "      action_ = np.argmax(qval_) \n",
    "      action = action_set[action_]\n",
    "      if display:\n",
    "        print('Move #: %s; Taking action: %s' % (i, action))\n",
    "      test_game.makeMove(action)\n",
    "      state_ = test_game.board.render_np().reshape(1,64) + np.random.rand(1,64)/10.0\n",
    "      state = torch.from_numpy(state_).float()\n",
    "      if display:\n",
    "        print(test_game.display())\n",
    "      reward = test_game.reward()\n",
    "      if reward != -1: #代表勝利（抵達終點）或落敗（掉入陷阱）\n",
    "        if reward > 0: #reward>0，代表成功抵達終點\n",
    "          status = 2 #將狀態設為2，跳出迴圈\n",
    "          if display:\n",
    "            print(\"Game won! Reward: %s\" %reward)\n",
    "          else: #掉入陷阱\n",
    "            status = 0 #將狀態設為0，跳出迴圈\n",
    "            if display:\n",
    "              print(\"Game LOST. Reward: %s\" %reward)\n",
    "      i += 1 #每移動一步，i就加1\n",
    "      if (i > 15): #若移動了15步，仍未取出勝利，則一樣視為落敗\n",
    "        if display:\n",
    "          print(\"Game lost; too many moves.\")\n",
    "        break    \n",
    "    win = True if status == 2 else False\n",
    "    print(win)\n",
    "    return win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3e74ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model, 'static')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c162081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model, 'random') #將游戲的生成模式改成random，再次測試模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eca7ca",
   "metadata": {},
   "source": [
    "## 📊 訓練結果可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52f88e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "losses = [] #使用串列將每一次的loss記錄下來，方便之後將loss的變化趨勢畫成圖\n",
    "for i in range(epochs):\n",
    "  game = Gridworld(size=4, mode='random')\n",
    "  state_ = game.board.render_np().reshape(1,64) + np.random.rand(1,64)/10.0 #將3階的狀態陣列（4x4x4）轉換成向量（長度為64），並將每個值都加上一些雜訊（很小的數值）。\t\n",
    "  state1 = torch.from_numpy(state_).float() #將NumPy陣列轉換成PyTorch張量，並存於state1中\n",
    "  status = 1 #用來追蹤遊戲是否仍在繼續（『1』代表仍在繼續）\n",
    "  while(status == 1):\n",
    "    qval = model(state1) #執行Q網路，取得所有動作的預測Q值\n",
    "    qval_ = qval.data.numpy() #將qval轉換成NumPy陣列\n",
    "    if (random.random() < epsilon): \n",
    "      action_ = np.random.randint(0,4) #隨機選擇一個動作（探索）\n",
    "    else:\n",
    "      action_ = np.argmax(qval_) #選擇Q值最大的動作（探索）        \n",
    "    action = action_set[action_] #將代表某動作的數字對應到makeMove()的英文字母\n",
    "    game.makeMove(action) #執行之前ε—貪婪策略所選出的動作 \n",
    "    state2_ = game.board.render_np().reshape(1,64) + np.random.rand(1,64)/10.0\n",
    "    state2 = torch.from_numpy(state2_).float() #動作執行完畢，取得遊戲的新狀態並轉換成張量\n",
    "    reward = game.reward()\n",
    "    with torch.no_grad(): \n",
    "      newQ = model(state2.reshape(1,64))\n",
    "    maxQ = torch.max(newQ) #將新狀態下所輸出的Q值向量中的最大值給記錄下來\n",
    "    if reward == -1:\n",
    "      Y = reward + (gamma * maxQ)  #計算訓練所用的目標Q值\n",
    "    else: #若reward不等於-1，代表遊戲已經結束，也就沒有下一個狀態了，因此目標Q值就等於回饋值\n",
    "      Y = reward\n",
    "    Y = torch.Tensor([Y]).detach() \n",
    "    X = qval.squeeze()[action_] #將演算法對執行的動作所預測的Q值存進X，並使用squeeze()將qval中維度為1的階去掉 (shape[1,4]會變成[4])\n",
    "    loss = loss_fn(X, Y) #計算目標Q值與預測Q值之間的誤差\n",
    "    if i%100 == 0:\n",
    "      print(i, loss.item())\n",
    "      clear_output(wait=True)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    state1 = state2\n",
    "    if abs(reward) == 10:       \n",
    "      status = 0 # 若 reward 的絕對值為10，代表遊戲已經分出勝負，所以設status為0  \n",
    "  losses.append(loss.item())\n",
    "  if epsilon > 0.1: \n",
    "    epsilon -= (1/epochs) #讓ε的值隨著訓練的進行而慢慢下降，直到0.1（還是要保留探索的動作）\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epochs\",fontsize=11)\n",
    "plt.ylabel(\"Loss\",fontsize=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa0f1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "losses = [] #使用串列將每一次的loss記錄下來，方便之後將loss的變化趨勢畫成圖\n",
    "for i in range(epochs):\n",
    "  game = Gridworld(size=4, mode='player')\n",
    "  state_ = game.board.render_np().reshape(1,64) + np.random.rand(1,64)/10.0 #將3階的狀態陣列（4x4x4）轉換成向量（長度為64），並將每個值都加上一些雜訊（很小的數值）。\t\n",
    "  state1 = torch.from_numpy(state_).float() #將NumPy陣列轉換成PyTorch張量，並存於state1中\n",
    "  status = 1 #用來追蹤遊戲是否仍在繼續（『1』代表仍在繼續）\n",
    "  while(status == 1):\n",
    "    qval = model(state1) #執行Q網路，取得所有動作的預測Q值\n",
    "    qval_ = qval.data.numpy() #將qval轉換成NumPy陣列\n",
    "    if (random.random() < epsilon): \n",
    "      action_ = np.random.randint(0,4) #隨機選擇一個動作（探索）\n",
    "    else:\n",
    "      action_ = np.argmax(qval_) #選擇Q值最大的動作（探索）        \n",
    "    action = action_set[action_] #將代表某動作的數字對應到makeMove()的英文字母\n",
    "    game.makeMove(action) #執行之前ε—貪婪策略所選出的動作 \n",
    "    state2_ = game.board.render_np().reshape(1,64) + np.random.rand(1,64)/10.0\n",
    "    state2 = torch.from_numpy(state2_).float() #動作執行完畢，取得遊戲的新狀態並轉換成張量\n",
    "    reward = game.reward()\n",
    "    with torch.no_grad(): \n",
    "      newQ = model(state2.reshape(1,64))\n",
    "    maxQ = torch.max(newQ) #將新狀態下所輸出的Q值向量中的最大值給記錄下來\n",
    "    if reward == -1:\n",
    "      Y = reward + (gamma * maxQ)  #計算訓練所用的目標Q值\n",
    "    else: #若reward不等於-1，代表遊戲已經結束，也就沒有下一個狀態了，因此目標Q值就等於回饋值\n",
    "      Y = reward\n",
    "    Y = torch.Tensor([Y]).detach() \n",
    "    X = qval.squeeze()[action_] #將演算法對執行的動作所預測的Q值存進X，並使用squeeze()將qval中維度為1的階去掉 (shape[1,4]會變成[4])\n",
    "    loss = loss_fn(X, Y) #計算目標Q值與預測Q值之間的誤差\n",
    "    if i%100 == 0:\n",
    "      print(i, loss.item())\n",
    "      clear_output(wait=True)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    state1 = state2\n",
    "    if abs(reward) == 10:       \n",
    "      status = 0 # 若 reward 的絕對值為10，代表遊戲已經分出勝負，所以設status為0  \n",
    "  losses.append(loss.item())\n",
    "  if epsilon > 0.1: \n",
    "    epsilon -= (1/epochs) #讓ε的值隨著訓練的進行而慢慢下降，直到0.1（還是要保留探索的動作）\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epochs\",fontsize=11)\n",
    "plt.ylabel(\"Loss\",fontsize=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8b6c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from Gridworld import Gridworld\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "L1 = 64 #輸入層的寬度\n",
    "L2 = 150 #第一隱藏層的寬度\n",
    "L3 = 100 #第二隱藏層的寬度\n",
    "L4 = 4 #輸出層的寬度\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(L1, L2), #第一隱藏層的shape \n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(L2, L3), #第二隱藏層的shape\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(L3,L4) #輸出層的shape\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss() #指定損失函數為MSE（均方誤差）\n",
    "learning_rate = 1e-3  #設定學習率\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) #指定優化器為Adam，其中model.parameters會傳回所有要優化的權重參數\n",
    "\n",
    "gamma = 0.9 #折扣因子\n",
    "epsilon = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c09422",
   "metadata": {},
   "source": [
    "## 📝 理解報告"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86db0ca0",
   "metadata": {},
   "source": [
    "\n",
    "### ✅ 基礎 DQN 架構說明：\n",
    "- 使用單層或雙層的神經網路對 Q-value 進行近似。\n",
    "- 使用 `epsilon-greedy` 方法進行探索與利用。\n",
    "- 每個 step 將經驗儲存進 replay buffer，並隨機抽樣進行訓練（打破資料相關性）。\n",
    "\n",
    "### ✅ Replay Buffer 作用：\n",
    "- 避免時間相依性（temporal correlation）\n",
    "- 提升樣本使用效率與訓練穩定性\n",
    "\n",
    "### ✅ 本作業環境：\n",
    "- 靜態 Gridworld，狀態與動作空間較小，適合 baseline 測試。\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
