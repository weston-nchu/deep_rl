{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3a78fb9",
   "metadata": {},
   "source": [
    "# HW4-1: Naive DQN for Static Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7639eb4",
   "metadata": {},
   "source": [
    "\n",
    "æœ¬ Notebook å±•ç¤ºåŸºç¤ DQNï¼ˆDeep Q-Learning Networkï¼‰çš„å¯¦ä½œï¼Œä¸¦æ‡‰ç”¨æ–¼ç°¡å–®çš„ Gridworld ç’°å¢ƒé€²è¡Œå­¸ç¿’ã€‚\n",
    "\n",
    "### åŒ…å«å…§å®¹ï¼š\n",
    "- ç’°å¢ƒåˆå§‹åŒ–ï¼ˆGridworldï¼‰\n",
    "- Naive DQN ç¶²è·¯æ¶æ§‹ï¼ˆä½¿ç”¨ PyTorchï¼‰\n",
    "- Experience Replay æ©Ÿåˆ¶\n",
    "- è¨“ç·´æµç¨‹èˆ‡æå¤±ç¹ªåœ–\n",
    "- ç†è§£èªªæ˜\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f745c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸‹è¼‰ Gridworld.py åŠ GridBoard.py (-q æ˜¯è¨­ç‚ºå®‰éœæ¨¡å¼)\n",
    "!curl -q https://github.com/DeepReinforcementLearning/DeepReinforcementLearningInAction/raw/master/Errata/Gridworld.py\n",
    "!curl -q https://github.com/DeepReinforcementLearning/DeepReinforcementLearningInAction/raw/master/Errata/GridBoard.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ad4a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Gridworld import Gridworld\n",
    "game = Gridworld(size=4, mode='static')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce23f3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad381b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.makeMove('d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa969765",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cd5a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d223751",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.board.render_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7351dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.board.render_np().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5237816b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from Gridworld import Gridworld\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "L1 = 64 #è¼¸å…¥å±¤çš„å¯¬åº¦\n",
    "L2 = 150 #ç¬¬ä¸€éš±è—å±¤çš„å¯¬åº¦\n",
    "L3 = 100 #ç¬¬äºŒéš±è—å±¤çš„å¯¬åº¦\n",
    "L4 = 4 #è¼¸å‡ºå±¤çš„å¯¬åº¦\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(L1, L2), #ç¬¬ä¸€éš±è—å±¤çš„shape \n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(L2, L3), #ç¬¬äºŒéš±è—å±¤çš„shape\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(L3,L4) #è¼¸å‡ºå±¤çš„shape\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss() #æŒ‡å®šæå¤±å‡½æ•¸ç‚ºMSEï¼ˆå‡æ–¹èª¤å·®ï¼‰\n",
    "learning_rate = 1e-3  #è¨­å®šå­¸ç¿’ç‡\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) #æŒ‡å®šå„ªåŒ–å™¨ç‚ºAdamï¼Œå…¶ä¸­model.parametersæœƒå‚³å›æ‰€æœ‰è¦å„ªåŒ–çš„æ¬Šé‡åƒæ•¸\n",
    "\n",
    "gamma = 0.9 #æŠ˜æ‰£å› å­\n",
    "epsilon = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d00565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_set = {\n",
    "\t0: 'u', #ã€0ã€ä»£è¡¨ã€å‘ä¸Šã€\n",
    "\t1: 'd', #ã€1ã€ä»£è¡¨ã€å‘ä¸‹ã€\n",
    "\t2: 'l', #ã€2ã€ä»£è¡¨ã€å‘å·¦ã€\n",
    "\t3: 'r' #ã€3ã€ä»£è¡¨ã€å‘å³ã€\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb521737",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "losses = [] #ä½¿ç”¨ä¸²åˆ—å°‡æ¯ä¸€æ¬¡çš„lossè¨˜éŒ„ä¸‹ä¾†ï¼Œæ–¹ä¾¿ä¹‹å¾Œå°‡lossçš„è®ŠåŒ–è¶¨å‹¢ç•«æˆåœ–\n",
    "for i in range(epochs):\n",
    "  game = Gridworld(size=4, mode='static')\n",
    "  state_ = game.board.render_np().reshape(1,64) + np.random.rand(1,64)/10.0 #å°‡3éšçš„ç‹€æ…‹é™£åˆ—ï¼ˆ4x4x4ï¼‰è½‰æ›æˆå‘é‡ï¼ˆé•·åº¦ç‚º64ï¼‰ï¼Œä¸¦å°‡æ¯å€‹å€¼éƒ½åŠ ä¸Šä¸€äº›é›œè¨Šï¼ˆå¾ˆå°çš„æ•¸å€¼ï¼‰ã€‚\t\n",
    "  state1 = torch.from_numpy(state_).float() #å°‡NumPyé™£åˆ—è½‰æ›æˆPyTorchå¼µé‡ï¼Œä¸¦å­˜æ–¼state1ä¸­\n",
    "  status = 1 #ç”¨ä¾†è¿½è¹¤éŠæˆ²æ˜¯å¦ä»åœ¨ç¹¼çºŒï¼ˆã€1ã€ä»£è¡¨ä»åœ¨ç¹¼çºŒï¼‰\n",
    "  while(status == 1):\n",
    "    qval = model(state1) #åŸ·è¡ŒQç¶²è·¯ï¼Œå–å¾—æ‰€æœ‰å‹•ä½œçš„é æ¸¬Qå€¼\n",
    "    qval_ = qval.data.numpy() #å°‡qvalè½‰æ›æˆNumPyé™£åˆ—\n",
    "    if (random.random() < epsilon): \n",
    "      action_ = np.random.randint(0,4) #éš¨æ©Ÿé¸æ“‡ä¸€å€‹å‹•ä½œï¼ˆæ¢ç´¢ï¼‰\n",
    "    else:\n",
    "      action_ = np.argmax(qval_) #é¸æ“‡Qå€¼æœ€å¤§çš„å‹•ä½œï¼ˆæ¢ç´¢ï¼‰        \n",
    "    action = action_set[action_] #å°‡ä»£è¡¨æŸå‹•ä½œçš„æ•¸å­—å°æ‡‰åˆ°makeMove()çš„è‹±æ–‡å­—æ¯\n",
    "    game.makeMove(action) #åŸ·è¡Œä¹‹å‰Îµâ€”è²ªå©ªç­–ç•¥æ‰€é¸å‡ºçš„å‹•ä½œ \n",
    "    state2_ = game.board.render_np().reshape(1,64) + np.random.rand(1,64)/10.0\n",
    "    state2 = torch.from_numpy(state2_).float() #å‹•ä½œåŸ·è¡Œå®Œç•¢ï¼Œå–å¾—éŠæˆ²çš„æ–°ç‹€æ…‹ä¸¦è½‰æ›æˆå¼µé‡\n",
    "    reward = game.reward()\n",
    "    with torch.no_grad(): \n",
    "      newQ = model(state2.reshape(1,64))\n",
    "    maxQ = torch.max(newQ) #å°‡æ–°ç‹€æ…‹ä¸‹æ‰€è¼¸å‡ºçš„Qå€¼å‘é‡ä¸­çš„æœ€å¤§å€¼çµ¦è¨˜éŒ„ä¸‹ä¾†\n",
    "    if reward == -1:\n",
    "      Y = reward + (gamma * maxQ)  #è¨ˆç®—è¨“ç·´æ‰€ç”¨çš„ç›®æ¨™Qå€¼\n",
    "    else: #è‹¥rewardä¸ç­‰æ–¼-1ï¼Œä»£è¡¨éŠæˆ²å·²ç¶“çµæŸï¼Œä¹Ÿå°±æ²’æœ‰ä¸‹ä¸€å€‹ç‹€æ…‹äº†ï¼Œå› æ­¤ç›®æ¨™Qå€¼å°±ç­‰æ–¼å›é¥‹å€¼\n",
    "      Y = reward\n",
    "    Y = torch.Tensor([Y]).detach() \n",
    "    X = qval.squeeze()[action_] #å°‡æ¼”ç®—æ³•å°åŸ·è¡Œçš„å‹•ä½œæ‰€é æ¸¬çš„Qå€¼å­˜é€²Xï¼Œä¸¦ä½¿ç”¨squeeze()å°‡qvalä¸­ç¶­åº¦ç‚º1çš„éšå»æ‰ (shape[1,4]æœƒè®Šæˆ[4])\n",
    "    loss = loss_fn(X, Y) #è¨ˆç®—ç›®æ¨™Qå€¼èˆ‡é æ¸¬Qå€¼ä¹‹é–“çš„èª¤å·®\n",
    "    if i%100 == 0:\n",
    "      print(i, loss.item())\n",
    "      clear_output(wait=True)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    state1 = state2\n",
    "    if abs(reward) == 10:       \n",
    "      status = 0 # è‹¥ reward çš„çµ•å°å€¼ç‚º10ï¼Œä»£è¡¨éŠæˆ²å·²ç¶“åˆ†å‡ºå‹è² ï¼Œæ‰€ä»¥è¨­statusç‚º0  \n",
    "  losses.append(loss.item())\n",
    "  if epsilon > 0.1: \n",
    "    epsilon -= (1/epochs) #è®“Îµçš„å€¼éš¨è‘—è¨“ç·´çš„é€²è¡Œè€Œæ…¢æ…¢ä¸‹é™ï¼Œç›´åˆ°0.1ï¼ˆé‚„æ˜¯è¦ä¿ç•™æ¢ç´¢çš„å‹•ä½œï¼‰\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epochs\",fontsize=11)\n",
    "plt.ylabel(\"Loss\",fontsize=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5b22f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.Tensor([2.0])\n",
    "m.requires_grad=True\n",
    "b = torch.Tensor([1.0]) \n",
    "b.requires_grad=True\n",
    "def linear_model(x,m,b):\n",
    "  y = m*x + b\n",
    "  return y\n",
    "y = linear_model(torch.Tensor([4.]),m,b)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c095539",
   "metadata": {},
   "source": [
    "## ğŸ” DQN è¨“ç·´æµç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec272438",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363716fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(): \n",
    "  y = linear_model(torch.Tensor([4.]),m,b)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c317f242",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9e3817",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = linear_model(torch.Tensor([4.]),m,b)\n",
    "y.backward()\n",
    "m.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83131a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a10fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, mode='static', display=True):\n",
    "    i = 0\n",
    "    test_game = Gridworld(size=4, mode=mode) #ç”¢ç”Ÿä¸€å ´æ¸¬è©¦éŠæˆ²\n",
    "    state_ = test_game.board.render_np().reshape(1,64) + np.random.rand(1,64)/10.0\n",
    "    state = torch.from_numpy(state_).float()\n",
    "    if display:\n",
    "      print(\"Initial State:\")\n",
    "      print(test_game.display())\n",
    "    status = 1\n",
    "    while(status == 1): #éŠæˆ²ä»åœ¨é€²è¡Œ\n",
    "      qval = model(state)\n",
    "      qval_ = qval.data.numpy()\n",
    "      action_ = np.argmax(qval_) \n",
    "      action = action_set[action_]\n",
    "      if display:\n",
    "        print('Move #: %s; Taking action: %s' % (i, action))\n",
    "      test_game.makeMove(action)\n",
    "      state_ = test_game.board.render_np().reshape(1,64) + np.random.rand(1,64)/10.0\n",
    "      state = torch.from_numpy(state_).float()\n",
    "      if display:\n",
    "        print(test_game.display())\n",
    "      reward = test_game.reward()\n",
    "      if reward != -1: #ä»£è¡¨å‹åˆ©ï¼ˆæŠµé”çµ‚é»ï¼‰æˆ–è½æ•—ï¼ˆæ‰å…¥é™·é˜±ï¼‰\n",
    "        if reward > 0: #reward>0ï¼Œä»£è¡¨æˆåŠŸæŠµé”çµ‚é»\n",
    "          status = 2 #å°‡ç‹€æ…‹è¨­ç‚º2ï¼Œè·³å‡ºè¿´åœˆ\n",
    "          if display:\n",
    "            print(\"Game won! Reward: %s\" %reward)\n",
    "          else: #æ‰å…¥é™·é˜±\n",
    "            status = 0 #å°‡ç‹€æ…‹è¨­ç‚º0ï¼Œè·³å‡ºè¿´åœˆ\n",
    "            if display:\n",
    "              print(\"Game LOST. Reward: %s\" %reward)\n",
    "      i += 1 #æ¯ç§»å‹•ä¸€æ­¥ï¼Œiå°±åŠ 1\n",
    "      if (i > 15): #è‹¥ç§»å‹•äº†15æ­¥ï¼Œä»æœªå–å‡ºå‹åˆ©ï¼Œå‰‡ä¸€æ¨£è¦–ç‚ºè½æ•—\n",
    "        if display:\n",
    "          print(\"Game lost; too many moves.\")\n",
    "        break    \n",
    "    win = True if status == 2 else False\n",
    "    print(win)\n",
    "    return win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3e74ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model, 'static')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c162081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model, 'random') #å°‡æ¸¸æˆ²çš„ç”Ÿæˆæ¨¡å¼æ”¹æˆrandomï¼Œå†æ¬¡æ¸¬è©¦æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eca7ca",
   "metadata": {},
   "source": [
    "## ğŸ“Š è¨“ç·´çµæœå¯è¦–åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52f88e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "losses = [] #ä½¿ç”¨ä¸²åˆ—å°‡æ¯ä¸€æ¬¡çš„lossè¨˜éŒ„ä¸‹ä¾†ï¼Œæ–¹ä¾¿ä¹‹å¾Œå°‡lossçš„è®ŠåŒ–è¶¨å‹¢ç•«æˆåœ–\n",
    "for i in range(epochs):\n",
    "  game = Gridworld(size=4, mode='random')\n",
    "  state_ = game.board.render_np().reshape(1,64) + np.random.rand(1,64)/10.0 #å°‡3éšçš„ç‹€æ…‹é™£åˆ—ï¼ˆ4x4x4ï¼‰è½‰æ›æˆå‘é‡ï¼ˆé•·åº¦ç‚º64ï¼‰ï¼Œä¸¦å°‡æ¯å€‹å€¼éƒ½åŠ ä¸Šä¸€äº›é›œè¨Šï¼ˆå¾ˆå°çš„æ•¸å€¼ï¼‰ã€‚\t\n",
    "  state1 = torch.from_numpy(state_).float() #å°‡NumPyé™£åˆ—è½‰æ›æˆPyTorchå¼µé‡ï¼Œä¸¦å­˜æ–¼state1ä¸­\n",
    "  status = 1 #ç”¨ä¾†è¿½è¹¤éŠæˆ²æ˜¯å¦ä»åœ¨ç¹¼çºŒï¼ˆã€1ã€ä»£è¡¨ä»åœ¨ç¹¼çºŒï¼‰\n",
    "  while(status == 1):\n",
    "    qval = model(state1) #åŸ·è¡ŒQç¶²è·¯ï¼Œå–å¾—æ‰€æœ‰å‹•ä½œçš„é æ¸¬Qå€¼\n",
    "    qval_ = qval.data.numpy() #å°‡qvalè½‰æ›æˆNumPyé™£åˆ—\n",
    "    if (random.random() < epsilon): \n",
    "      action_ = np.random.randint(0,4) #éš¨æ©Ÿé¸æ“‡ä¸€å€‹å‹•ä½œï¼ˆæ¢ç´¢ï¼‰\n",
    "    else:\n",
    "      action_ = np.argmax(qval_) #é¸æ“‡Qå€¼æœ€å¤§çš„å‹•ä½œï¼ˆæ¢ç´¢ï¼‰        \n",
    "    action = action_set[action_] #å°‡ä»£è¡¨æŸå‹•ä½œçš„æ•¸å­—å°æ‡‰åˆ°makeMove()çš„è‹±æ–‡å­—æ¯\n",
    "    game.makeMove(action) #åŸ·è¡Œä¹‹å‰Îµâ€”è²ªå©ªç­–ç•¥æ‰€é¸å‡ºçš„å‹•ä½œ \n",
    "    state2_ = game.board.render_np().reshape(1,64) + np.random.rand(1,64)/10.0\n",
    "    state2 = torch.from_numpy(state2_).float() #å‹•ä½œåŸ·è¡Œå®Œç•¢ï¼Œå–å¾—éŠæˆ²çš„æ–°ç‹€æ…‹ä¸¦è½‰æ›æˆå¼µé‡\n",
    "    reward = game.reward()\n",
    "    with torch.no_grad(): \n",
    "      newQ = model(state2.reshape(1,64))\n",
    "    maxQ = torch.max(newQ) #å°‡æ–°ç‹€æ…‹ä¸‹æ‰€è¼¸å‡ºçš„Qå€¼å‘é‡ä¸­çš„æœ€å¤§å€¼çµ¦è¨˜éŒ„ä¸‹ä¾†\n",
    "    if reward == -1:\n",
    "      Y = reward + (gamma * maxQ)  #è¨ˆç®—è¨“ç·´æ‰€ç”¨çš„ç›®æ¨™Qå€¼\n",
    "    else: #è‹¥rewardä¸ç­‰æ–¼-1ï¼Œä»£è¡¨éŠæˆ²å·²ç¶“çµæŸï¼Œä¹Ÿå°±æ²’æœ‰ä¸‹ä¸€å€‹ç‹€æ…‹äº†ï¼Œå› æ­¤ç›®æ¨™Qå€¼å°±ç­‰æ–¼å›é¥‹å€¼\n",
    "      Y = reward\n",
    "    Y = torch.Tensor([Y]).detach() \n",
    "    X = qval.squeeze()[action_] #å°‡æ¼”ç®—æ³•å°åŸ·è¡Œçš„å‹•ä½œæ‰€é æ¸¬çš„Qå€¼å­˜é€²Xï¼Œä¸¦ä½¿ç”¨squeeze()å°‡qvalä¸­ç¶­åº¦ç‚º1çš„éšå»æ‰ (shape[1,4]æœƒè®Šæˆ[4])\n",
    "    loss = loss_fn(X, Y) #è¨ˆç®—ç›®æ¨™Qå€¼èˆ‡é æ¸¬Qå€¼ä¹‹é–“çš„èª¤å·®\n",
    "    if i%100 == 0:\n",
    "      print(i, loss.item())\n",
    "      clear_output(wait=True)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    state1 = state2\n",
    "    if abs(reward) == 10:       \n",
    "      status = 0 # è‹¥ reward çš„çµ•å°å€¼ç‚º10ï¼Œä»£è¡¨éŠæˆ²å·²ç¶“åˆ†å‡ºå‹è² ï¼Œæ‰€ä»¥è¨­statusç‚º0  \n",
    "  losses.append(loss.item())\n",
    "  if epsilon > 0.1: \n",
    "    epsilon -= (1/epochs) #è®“Îµçš„å€¼éš¨è‘—è¨“ç·´çš„é€²è¡Œè€Œæ…¢æ…¢ä¸‹é™ï¼Œç›´åˆ°0.1ï¼ˆé‚„æ˜¯è¦ä¿ç•™æ¢ç´¢çš„å‹•ä½œï¼‰\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epochs\",fontsize=11)\n",
    "plt.ylabel(\"Loss\",fontsize=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa0f1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "losses = [] #ä½¿ç”¨ä¸²åˆ—å°‡æ¯ä¸€æ¬¡çš„lossè¨˜éŒ„ä¸‹ä¾†ï¼Œæ–¹ä¾¿ä¹‹å¾Œå°‡lossçš„è®ŠåŒ–è¶¨å‹¢ç•«æˆåœ–\n",
    "for i in range(epochs):\n",
    "  game = Gridworld(size=4, mode='player')\n",
    "  state_ = game.board.render_np().reshape(1,64) + np.random.rand(1,64)/10.0 #å°‡3éšçš„ç‹€æ…‹é™£åˆ—ï¼ˆ4x4x4ï¼‰è½‰æ›æˆå‘é‡ï¼ˆé•·åº¦ç‚º64ï¼‰ï¼Œä¸¦å°‡æ¯å€‹å€¼éƒ½åŠ ä¸Šä¸€äº›é›œè¨Šï¼ˆå¾ˆå°çš„æ•¸å€¼ï¼‰ã€‚\t\n",
    "  state1 = torch.from_numpy(state_).float() #å°‡NumPyé™£åˆ—è½‰æ›æˆPyTorchå¼µé‡ï¼Œä¸¦å­˜æ–¼state1ä¸­\n",
    "  status = 1 #ç”¨ä¾†è¿½è¹¤éŠæˆ²æ˜¯å¦ä»åœ¨ç¹¼çºŒï¼ˆã€1ã€ä»£è¡¨ä»åœ¨ç¹¼çºŒï¼‰\n",
    "  while(status == 1):\n",
    "    qval = model(state1) #åŸ·è¡ŒQç¶²è·¯ï¼Œå–å¾—æ‰€æœ‰å‹•ä½œçš„é æ¸¬Qå€¼\n",
    "    qval_ = qval.data.numpy() #å°‡qvalè½‰æ›æˆNumPyé™£åˆ—\n",
    "    if (random.random() < epsilon): \n",
    "      action_ = np.random.randint(0,4) #éš¨æ©Ÿé¸æ“‡ä¸€å€‹å‹•ä½œï¼ˆæ¢ç´¢ï¼‰\n",
    "    else:\n",
    "      action_ = np.argmax(qval_) #é¸æ“‡Qå€¼æœ€å¤§çš„å‹•ä½œï¼ˆæ¢ç´¢ï¼‰        \n",
    "    action = action_set[action_] #å°‡ä»£è¡¨æŸå‹•ä½œçš„æ•¸å­—å°æ‡‰åˆ°makeMove()çš„è‹±æ–‡å­—æ¯\n",
    "    game.makeMove(action) #åŸ·è¡Œä¹‹å‰Îµâ€”è²ªå©ªç­–ç•¥æ‰€é¸å‡ºçš„å‹•ä½œ \n",
    "    state2_ = game.board.render_np().reshape(1,64) + np.random.rand(1,64)/10.0\n",
    "    state2 = torch.from_numpy(state2_).float() #å‹•ä½œåŸ·è¡Œå®Œç•¢ï¼Œå–å¾—éŠæˆ²çš„æ–°ç‹€æ…‹ä¸¦è½‰æ›æˆå¼µé‡\n",
    "    reward = game.reward()\n",
    "    with torch.no_grad(): \n",
    "      newQ = model(state2.reshape(1,64))\n",
    "    maxQ = torch.max(newQ) #å°‡æ–°ç‹€æ…‹ä¸‹æ‰€è¼¸å‡ºçš„Qå€¼å‘é‡ä¸­çš„æœ€å¤§å€¼çµ¦è¨˜éŒ„ä¸‹ä¾†\n",
    "    if reward == -1:\n",
    "      Y = reward + (gamma * maxQ)  #è¨ˆç®—è¨“ç·´æ‰€ç”¨çš„ç›®æ¨™Qå€¼\n",
    "    else: #è‹¥rewardä¸ç­‰æ–¼-1ï¼Œä»£è¡¨éŠæˆ²å·²ç¶“çµæŸï¼Œä¹Ÿå°±æ²’æœ‰ä¸‹ä¸€å€‹ç‹€æ…‹äº†ï¼Œå› æ­¤ç›®æ¨™Qå€¼å°±ç­‰æ–¼å›é¥‹å€¼\n",
    "      Y = reward\n",
    "    Y = torch.Tensor([Y]).detach() \n",
    "    X = qval.squeeze()[action_] #å°‡æ¼”ç®—æ³•å°åŸ·è¡Œçš„å‹•ä½œæ‰€é æ¸¬çš„Qå€¼å­˜é€²Xï¼Œä¸¦ä½¿ç”¨squeeze()å°‡qvalä¸­ç¶­åº¦ç‚º1çš„éšå»æ‰ (shape[1,4]æœƒè®Šæˆ[4])\n",
    "    loss = loss_fn(X, Y) #è¨ˆç®—ç›®æ¨™Qå€¼èˆ‡é æ¸¬Qå€¼ä¹‹é–“çš„èª¤å·®\n",
    "    if i%100 == 0:\n",
    "      print(i, loss.item())\n",
    "      clear_output(wait=True)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    state1 = state2\n",
    "    if abs(reward) == 10:       \n",
    "      status = 0 # è‹¥ reward çš„çµ•å°å€¼ç‚º10ï¼Œä»£è¡¨éŠæˆ²å·²ç¶“åˆ†å‡ºå‹è² ï¼Œæ‰€ä»¥è¨­statusç‚º0  \n",
    "  losses.append(loss.item())\n",
    "  if epsilon > 0.1: \n",
    "    epsilon -= (1/epochs) #è®“Îµçš„å€¼éš¨è‘—è¨“ç·´çš„é€²è¡Œè€Œæ…¢æ…¢ä¸‹é™ï¼Œç›´åˆ°0.1ï¼ˆé‚„æ˜¯è¦ä¿ç•™æ¢ç´¢çš„å‹•ä½œï¼‰\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epochs\",fontsize=11)\n",
    "plt.ylabel(\"Loss\",fontsize=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8b6c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from Gridworld import Gridworld\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "L1 = 64 #è¼¸å…¥å±¤çš„å¯¬åº¦\n",
    "L2 = 150 #ç¬¬ä¸€éš±è—å±¤çš„å¯¬åº¦\n",
    "L3 = 100 #ç¬¬äºŒéš±è—å±¤çš„å¯¬åº¦\n",
    "L4 = 4 #è¼¸å‡ºå±¤çš„å¯¬åº¦\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(L1, L2), #ç¬¬ä¸€éš±è—å±¤çš„shape \n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(L2, L3), #ç¬¬äºŒéš±è—å±¤çš„shape\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(L3,L4) #è¼¸å‡ºå±¤çš„shape\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss() #æŒ‡å®šæå¤±å‡½æ•¸ç‚ºMSEï¼ˆå‡æ–¹èª¤å·®ï¼‰\n",
    "learning_rate = 1e-3  #è¨­å®šå­¸ç¿’ç‡\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) #æŒ‡å®šå„ªåŒ–å™¨ç‚ºAdamï¼Œå…¶ä¸­model.parametersæœƒå‚³å›æ‰€æœ‰è¦å„ªåŒ–çš„æ¬Šé‡åƒæ•¸\n",
    "\n",
    "gamma = 0.9 #æŠ˜æ‰£å› å­\n",
    "epsilon = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c09422",
   "metadata": {},
   "source": [
    "## ğŸ“ ç†è§£å ±å‘Š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86db0ca0",
   "metadata": {},
   "source": [
    "\n",
    "### âœ… åŸºç¤ DQN æ¶æ§‹èªªæ˜ï¼š\n",
    "- ä½¿ç”¨å–®å±¤æˆ–é›™å±¤çš„ç¥ç¶“ç¶²è·¯å° Q-value é€²è¡Œè¿‘ä¼¼ã€‚\n",
    "- ä½¿ç”¨ `epsilon-greedy` æ–¹æ³•é€²è¡Œæ¢ç´¢èˆ‡åˆ©ç”¨ã€‚\n",
    "- æ¯å€‹ step å°‡ç¶“é©—å„²å­˜é€² replay bufferï¼Œä¸¦éš¨æ©ŸæŠ½æ¨£é€²è¡Œè¨“ç·´ï¼ˆæ‰“ç ´è³‡æ–™ç›¸é—œæ€§ï¼‰ã€‚\n",
    "\n",
    "### âœ… Replay Buffer ä½œç”¨ï¼š\n",
    "- é¿å…æ™‚é–“ç›¸ä¾æ€§ï¼ˆtemporal correlationï¼‰\n",
    "- æå‡æ¨£æœ¬ä½¿ç”¨æ•ˆç‡èˆ‡è¨“ç·´ç©©å®šæ€§\n",
    "\n",
    "### âœ… æœ¬ä½œæ¥­ç’°å¢ƒï¼š\n",
    "- éœæ…‹ Gridworldï¼Œç‹€æ…‹èˆ‡å‹•ä½œç©ºé–“è¼ƒå°ï¼Œé©åˆ baseline æ¸¬è©¦ã€‚\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
